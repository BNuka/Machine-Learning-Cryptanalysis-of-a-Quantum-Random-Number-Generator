{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, MaxPooling1D, Flatten\n",
    "from keras.layers import LSTM,Convolution1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate pseudo-random numbers with congruential random number generator.\n",
    "Only excecute once since data will be saved to hard disk.\n",
    "'''\n",
    "m_=24 # try with m_=16 first to see if the network is learning\n",
    "\n",
    "def rngint(nbit=8):\n",
    "    return int(rng()*(2**nbit))\n",
    "\n",
    "def rng(m=2**m_, a=1103515245, c=12345):\n",
    "    rng.current = (a*rng.current + c) % m\n",
    "    return float(rng.current)/m\n",
    "\n",
    "# setting the seed\n",
    "rng.current = 10\n",
    "\n",
    "data = np.array([rngint() for i in range(10000000)])\n",
    "data.tofile('CRNG_10M_M24.bin')\n",
    "# print (data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate another sequence for testing\n",
    "'''\n",
    "# setting the seed\n",
    "rng.current = 139 # maybe use another seed\n",
    "\n",
    "data = np.array([rngint() for i in range(10000000)])\n",
    "data.tofile('CRNG_10M_M24_test.bin')\n",
    "# print (data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.fromfile('CRNG_10M_M24.bin', dtype='<i8')\n",
    "text = list(data)\n",
    "text = list(map(str,text))\n",
    "print (data.shape)\n",
    "print (data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Treating each number as a \"word\". Creating a dictionary.\n",
    "data = data.astype(np.str)\n",
    "chars = sorted(list(set(data)))\n",
    "print(chars)\n",
    "del data\n",
    "print('Total words:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of input. Treating each input that consists of 100 \"words\" as a \"sentence\".\n",
    "maxlen = 10\n",
    "# Distance between 2 consecutive \"sentences\"\n",
    "step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: (i + maxlen)])\n",
    "    next_chars.append(text[(i + maxlen)])\n",
    "print('Number of sentences:', len(sentences))\n",
    "\n",
    "\n",
    "print('Start vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):    \n",
    "    for t, char in enumerate(sentence):        \n",
    "        X[i, t, char_indices[char]] = 1    \n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "print('Done vectorization!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build the RCNN model\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(filters=64, kernel_size=9, padding='same', activation='relu', input_shape=(maxlen, len(chars))))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Convolution1D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print (model.summary())\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "monitoring = ModelCheckpoint('weights_a20_125M.hdf5', monitor='val_loss', verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=100, batch_size=128, validation_split=0.2, verbose=1, callbacks=[early_stopping,monitoring])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights_a20_125M.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "data = np.fromfile('CRNG_10M_M24_test.bin', dtype='<i8')\n",
    "test_len=int(0.2*data.shape[0]) # 5 test sets\n",
    "\n",
    "test1 = data[:test_len]\n",
    "test2 = data[test_len:test_len*2]\n",
    "test3 = data[test_len*2:test_len*3]\n",
    "test4 = data[test_len*3:test_len*4]\n",
    "test5 = data[test_len*4:]\n",
    "\n",
    "test1 = list(test1)\n",
    "test2 = list(test2)\n",
    "test3 = list(test3)\n",
    "test4 = list(test4)\n",
    "test5 = list(test5)\n",
    "\n",
    "test1 = list(map(str,test1))\n",
    "test2 = list(map(str,test2))\n",
    "test3 = list(map(str,test3))\n",
    "test4 = list(map(str,test4))\n",
    "test5 = list(map(str,test5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [test1,test2,test3,test4,test5]\n",
    "del test1\n",
    "del test2\n",
    "del test3\n",
    "del test4\n",
    "del test5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for test in tests:\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    maxlen = 10\n",
    "    step = 10 # increase to downsample test set\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(test) - maxlen, step):\n",
    "        sentences.append(test[i: (i + maxlen)])\n",
    "        next_chars.append(test[(i + maxlen)])\n",
    "    \n",
    "    Xt = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "    yt = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        \n",
    "        for t, char in enumerate(sentence):        \n",
    "            Xt[i, t, char_indices[char]] = 1       \n",
    "        yt[i, char_indices[next_chars[i]]] = 1\n",
    "    n_true = 0\n",
    "    diversity = 1\n",
    "    \n",
    "    batch_size = 1000\n",
    "    nb_batch = int(Xt.shape[0]/batch_size)\n",
    "    \n",
    "    for i in range(1,nb_batch+1):\n",
    "        if i % 100 == 0:\n",
    "            print (\"Processed %d predictions, %d correct!\" % (i*batch_size,n_true))\n",
    "        x = Xt[i*batch_size:(i+1)*batch_size]\n",
    "        preds = model.predict(x, verbose=0)\n",
    "        pred_next_indexes = list(np.argmax(preds,axis=1))\n",
    "        pred_next_chars = [indices_char[next_index] for next_index in pred_next_indexes]        \n",
    "        y_pred += pred_next_chars\n",
    "        \n",
    "        true_next_indexes = list(np.argmax(yt[i*batch_size:(i+1)*batch_size],axis=1))\n",
    "        true_next_chars = [indices_char[next_index] for next_index in true_next_indexes]\n",
    "        y_true += true_next_chars\n",
    "        \n",
    "        n_true += np.sum(np.array(pred_next_chars)==np.array(true_next_chars))\n",
    "        \n",
    "    y_true = list(map(int,y_true))\n",
    "    y_pred = list(map(int,y_pred))\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    mse = np.mean(np.square(y_true-y_pred))\n",
    "    print ('mse',mse)\n",
    "    print ('Total {0:d} predictions, {1:d} correct, accuracy is {2:.2f}%.'.format(yt.shape[0],n_true,float(n_true)/yt.shape[0]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
