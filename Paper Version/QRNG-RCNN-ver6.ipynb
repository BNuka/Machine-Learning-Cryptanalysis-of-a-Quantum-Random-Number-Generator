{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, MaxPooling1D, Flatten\n",
    "from keras.layers import LSTM,Convolution1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = []\n",
    "filenames = ['./Sec_III_A/LPF2.dat']\n",
    "\n",
    "for filename in filenames:\n",
    "    data = np.fromfile(filename, dtype='>i2')    \n",
    "    data = data[2:]   # exclude first two header values\n",
    "    data = data >> 3  # shift right by 3 bits  \n",
    "    alldata.append(data)\n",
    "\n",
    "mu, sigma = np.mean(data), np.std(data)\n",
    "print mu, sigma\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(data, 1024, normed=1, facecolor='green', alpha=0.75)\n",
    "\n",
    "# add a 'best fit' line\n",
    "y = mlab.normpdf( bins, mu, sigma)\n",
    "l = plt.plot(bins, y, 'r--', linewidth=1)\n",
    "\n",
    "plt.xlabel('Bins')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(r'$\\mathrm{Histogram\\ of\\ QNRG\\ Electrical\\ raw\\ data:}\\ \\mu=%.2f,\\ \\sigma=%.2f$' % (mu,sigma))\n",
    "#plt.axis([-10000, 10000, 0, 0.002])\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del data    \n",
    "alldata = np.concatenate(alldata)    \n",
    "print alldata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = alldata[:5000000]\n",
    "test1 = alldata[5000000:6000000]\n",
    "test2 = alldata[6000000:7000000]\n",
    "test3 = alldata[7000000:8000000]\n",
    "test4 = alldata[8000000:9000000]\n",
    "test5 = alldata[9000000:]\n",
    "text = list(text)\n",
    "test1 = list(test1)\n",
    "test2 = list(test2)\n",
    "test3 = list(test3)\n",
    "test4 = list(test4)\n",
    "test5 = list(test5)\n",
    "\n",
    "text = map(str,text)\n",
    "test1 = map(str,test1)\n",
    "test2 = map(str,test2)\n",
    "test3 = map(str,test3)\n",
    "test4 = map(str,test4)\n",
    "test5 = map(str,test5)\n",
    "\n",
    "print len(text), len(test1), len(test5)\n",
    "print text[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating each number as a \"word\". Creating a dictionary.\n",
    "alldata = alldata.astype(np.str)\n",
    "chars = sorted(list(set(alldata)))\n",
    "\n",
    "print(chars)\n",
    "del alldata\n",
    "print('Total words:', len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of input. Treating each input that consists of 100 \"words\" as a \"sentence\".\n",
    "maxlen = 100\n",
    "# Distance between 2 consecutive \"sentences\". Can be set larger for faster training\n",
    "step = 3\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: (i + maxlen)])\n",
    "    next_chars.append(text[(i + maxlen)])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "\n",
    "print('Start vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    #print ('sentence', sentence)\n",
    "    for t, char in enumerate(sentence):        \n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    #print next_chars[i]\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "print('Done vectorization!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build the RCNN model\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(nb_filter=64, filter_length=9, border_mode='same', activation='relu', input_shape=(maxlen, len(chars))))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Convolution1D(nb_filter=128, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "monitoring = ModelCheckpoint('weights_E3a_ch0_ver6.hdf5', monitor='val_loss', verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, nb_epoch=50, batch_size=128, validation_split=0.2, verbose=1, callbacks=[early_stopping,monitoring])\n",
    "model.load_weights('weights_E3a_ch0_ver6.hdf5')\n",
    "#os.remove('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Running test\n",
    "tests = [test1,test2,test3,test4,test5]\n",
    "del test1\n",
    "del test2\n",
    "del test3\n",
    "del test4\n",
    "del test5\n",
    "for test in tests:\n",
    "    maxlen = 100\n",
    "    step = 1\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(test) - maxlen, step):\n",
    "        sentences.append(test[i: (i + maxlen)])\n",
    "        next_chars.append(test[(i + maxlen)])\n",
    "    print('nb sequences:', len(sentences))\n",
    "\n",
    "    print('Vectorization...')\n",
    "    Xt = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "    yt = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        \n",
    "        for t, char in enumerate(sentence):        \n",
    "            Xt[i, t, char_indices[char]] = 1       \n",
    "        yt[i, char_indices[next_chars[i]]] = 1\n",
    "    n_true = 0\n",
    "    diversity = 1\n",
    "    for i,x in enumerate(Xt):\n",
    "        if i % 100000 == 0:\n",
    "            print (\"Processed %d %d\" % (i,n_true))\n",
    "        x = x.reshape(1,maxlen,-1)\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = np.argmax(preds)      \n",
    "        next_char = indices_char[next_index]\n",
    "        if next_char == indices_char[np.argmax(yt[i])]:\n",
    "            n_true += 1\n",
    "    print (\"%d_%d_%.5f\" % (n_true,yt.shape[0],(float(n_true)/yt.shape[0])))        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
